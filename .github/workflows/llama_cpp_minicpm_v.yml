name: Build llama.cpp (MiniCPM-V)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  get_short_commit_hash:
    runs-on: ubuntu-latest
    outputs:
      hash: ${{ steps.short_commit_hash.outputs.short }}
    steps:
      - name: Prepare commit hash
        id: commit_hash
        run: |
          git clone https://github.com/Achazwl/llama.cpp cache -b feat-minicpmv --depth 1
          cd cache
          echo "hash=$(git rev-parse HEAD)" >> $GITHUB_OUTPUT
          cd ..
          rm -rf cache
      - name: Get short commit hash
        id: short_commit_hash
        uses: prompt/actions-commit-hash@v3
        with:
          commit: ${{ steps.commit_hash.outputs.hash }}
  build_intel_mkl_avx2_linux:
    needs: get_short_commit_hash
    runs-on: ubuntu-20.04
    steps:
      - name: Install Deps
        run: |
          wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
          echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
          sudo apt-get update
          sudo apt-get install ccache intel-basekit intel-oneapi-mkl intel-oneapi-mkl-devel ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: Achazwl/llama.cpp
          submodules: recursive
          ref: feat-minicpmv
      - name: Build
        run: |
          mkdir build
          cd build
          source /opt/intel/oneapi/setvars.sh
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_NATIVE=ON -DLLAMA_AVX2=ON
          cmake --build . --config Release -j $(nproc)
      - name: Pack
        run: |
          mkdir -p build/dist 
          cd build/bin/
          zip ../dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-intel-mkl-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: intel-mkl-avx2-linux
          path: build/dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-intel-mkl-x64.zip
  build_sycl_avx2_linux:
    needs: get_short_commit_hash
    runs-on: ubuntu-latest
    steps:
      - name: Install Deps
        run: |
          wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
          echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
          sudo apt-get update
          sudo apt-get install ccache intel-basekit intel-oneapi-mkl intel-oneapi-mkl-devel ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: Achazwl/llama.cpp
          submodules: recursive
          ref: feat-minicpmv
      - name: Build
        run: |
          mkdir build
          cd build
          source /opt/intel/oneapi/setvars.sh
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_SYCL=ON -DLLAMA_SYCL_F16=ON -DLLAMA_AVX2=ON
          cmake --build . --config Release -j $(nproc)
      - name: Pack
        run: |
          mkdir -p build/dist
          cd build/bin/
          zip ../dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-sycl-x64.zip ./* ../*.so ../build/examples/**/*.so
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: sycl-avx2-linux
          path: build/dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-sycl-x64.zip
  build_cu121_avx2_linux:
    needs: get_short_commit_hash
    runs-on: ubuntu-latest
    steps:
      - name: Install CUDA 12.1
        id: install-cuda
        uses: Jimver/cuda-toolkit@v0.2.13
        with:
          cuda: "12.1.1"
          method: network
      - name: Install Deps
        run: sudo apt-get install ccache ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: Achazwl/llama.cpp
          submodules: recursive
          ref: feat-minicpmv
      - name: Build
        run: |
          mkdir build
          cd build
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON -DLLAMA_CUBLAS=ON -DLLAMA_CUDA_F16=true -DLLAMA_AVX2=ON
          cmake --build . --config Release -j $(nproc)
      - name: Pack
        run: |
          mkdir -p build/dist
          cd build/bin/
          zip ../dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-cublas-cu121-x64.zip ./* ../*.so ../build/examples/**/*.so
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: cu121-avx2-linux
          path: build/dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-cublas-cu121-x64.zip
  build_cu121_avx512_win:
    needs: get_short_commit_hash
    runs-on: windows-latest
    steps:
      - name: Setup VS Dev Environment
        uses: seanmiddleditch/gha-setup-vsdevenv@v4
      - name: Install CUDA 12.1
        id: install-cuda
        uses: Jimver/cuda-toolkit@v0.2.13
        with:
          cuda: "12.1.1"
          method: network
          sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'
      - name: Setup ccache
        uses: Chocobo1/setup-ccache-action@v1
        with:
          windows_compile_environment: msvc
      - name: Install Ninja
        uses: urkle/action-get-ninja@v1
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: Achazwl/llama.cpp
          submodules: recursive
          ref: feat-minicpmv
      - name: Build
        run: |
          mkdir build
          cd build
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DLLAMA_CUDA=ON -DLLAMA_CUDA_F16=true -DLLAMA_AVX512=ON
          cmake --build . --config Release -j $env:NUMBER_OF_PROCESSORS
      - name: Pack
        run: |
          cd build\bin\
          7z a ..\dist\llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx512-cublas-cu121-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: cu121-avx512-win
          path: build/dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx512-cublas-cu121-x64.zip
  build_cu113_avx2_win:
    needs: get_short_commit_hash
    runs-on: windows-2019
    steps:
      - name: Setup VS Dev Environment
        uses: seanmiddleditch/gha-setup-vsdevenv@v4
      - name: Install CUDA 11.3
        id: install-cuda
        uses: Jimver/cuda-toolkit@v0.2.13
        with:
          cuda: "11.3.1"
          method: network
          sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'
      - name: Setup ccache
        uses: Chocobo1/setup-ccache-action@v1
        with:
          windows_compile_environment: msvc
      - name: Install Ninja
        uses: urkle/action-get-ninja@v1
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: Achazwl/llama.cpp
          submodules: recursive
          ref: feat-minicpmv
      - name: Build
        run: |
          mkdir build
          cd build
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DLLAMA_CUDA=ON -DLLAMA_CUDA_KQUANTS_ITER=1 -DCMAKE_CUDA_ARCHITECTURES="35" -DLLAMA_AVX2=ON
          cmake --build . --config Release -j $env:NUMBER_OF_PROCESSORS
      - name: Pack
        run: |
          cd build\bin\
          7z a ..\dist\llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx2-cublas-cu113-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: cu113-avx2-win
          path: build/dist/llama-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx2-cublas-cu113-x64.zip
  release:
    runs-on: ubuntu-latest
    needs: [get_short_commit_hash, build_intel_mkl_avx2_linux, build_sycl_avx2_linux, build_cu121_avx2_linux, build_cu121_avx512_win, build_cu113_avx2_win]
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v3
        with:
          path: |
            dist/
      - name: Prepare Release
        run: |
          cd dist
          mv -f intel-mkl-avx2-linux/* .
          mv -f sycl-avx2-linux/* .
          mv -f cu121-avx2-linux/* .
          mv -f cu121-avx512-win/* .
          mv -f cu113-avx2-win/* .
          rm -rf intel-mkl-avx2-linux sycl-avx2-linux cu121-avx2-linux cu121-avx512-win cu113-avx2-win
      - name: Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: llama_cpp-minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }}
          name: Binary build for llama.cpp (minicpm-v-${{ needs.get_short_commit_hash.outputs.hash }})
          body: |
            This release contains the x86_64 binary (MiniCPM-V version) of [llama.cpp](https://github.com/Achazwl/llama.cpp/tree/feat-minicpmv).
          files: dist/*
          fail_on_unmatched_files: true
